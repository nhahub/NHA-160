{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19ababa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "import concurrent.futures\n",
    "from dotenv import load_dotenv\n",
    "import tenacity  # for retrying failed requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import openpyxl\n",
    "import pycountry\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315345c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"âŒ API_KEY Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù…Ù„Ù .env\")\n",
    "\n",
    "OUT_DIR = \"data\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_FILE = os.path.join(OUT_DIR, \"movies_all.ndjson\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738a2c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch data from TMDB API with retries and exponential backoff\n",
    "\n",
    "def fetch(endpoint, params=None, retries=10, backoff=2):\n",
    "    base_url = \"https://api.themoviedb.org/3\"\n",
    "    params = params or {}\n",
    "    params[\"api_key\"] = API_KEY\n",
    "    url = f\"{base_url}{endpoint}\"\n",
    "\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except (requests.exceptions.ConnectionError,\n",
    "                requests.exceptions.Timeout) as e:\n",
    "            wait = backoff * (i + 1)\n",
    "            logger.warning(f\"âš ï¸ Network error: {e}. Retrying in {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Fatal error: {e}\")\n",
    "            raise\n",
    "    raise Exception(\"Failed after retries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d23a39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate date ranges (yearly or half-yearly)\n",
    "\n",
    "# step can be \"year\" or \"half\"\n",
    "def generate_date_ranges(start_year=1900, end_year=2025, step=\"year\"):\n",
    "    ranges = []\n",
    "    for y in range(start_year, end_year + 1):\n",
    "        if step == \"year\":\n",
    "            start = f\"{y}-01-01\"\n",
    "            end = f\"{y}-12-31\"\n",
    "            ranges.append((start, end))\n",
    "        elif step == \"half\":\n",
    "            # First half\n",
    "            ranges.append((f\"{y}-01-01\", f\"{y}-06-30\"))\n",
    "            # Second half\n",
    "            ranges.append((f\"{y}-07-01\", f\"{y}-12-31\"))\n",
    "    return ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate quarter date ranges\n",
    "\n",
    "def generate_quarter_ranges(start_year=1900, end_year=2025):\n",
    "    quarters = [\n",
    "        (\"01-01\", \"03-31\"),\n",
    "        (\"04-01\", \"06-30\"),\n",
    "        (\"07-01\", \"09-30\"),\n",
    "        (\"10-01\", \"12-31\"),\n",
    "    ]\n",
    "    ranges = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for q_start, q_end in quarters:\n",
    "            start = f\"{year}-{q_start}\"\n",
    "            end = f\"{year}-{q_end}\"\n",
    "            ranges.append((start, end))\n",
    "    return ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b98c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate monthly date ranges\n",
    "\n",
    "def generate_monthly_ranges(start_year=1900, end_year=2025):\n",
    "    import calendar\n",
    "    ranges = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(1, 13):\n",
    "            start = f\"{year}-{month:02d}-01\"\n",
    "            last_day = calendar.monthrange(\n",
    "                year, month)[1]  # Get last day of the month\n",
    "            end = f\"{year}-{month:02d}-{last_day}\"\n",
    "            ranges.append((start, end))\n",
    "    return ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61e6576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to discover movies with pagination and optional date range filtering\n",
    "\n",
    "def discover_movies(pages=5, date_range=None, sort_by=\"popularity.desc\"):\n",
    "    results = []\n",
    "    params = {\"sort_by\": sort_by}\n",
    "\n",
    "    if date_range:\n",
    "        params[\"primary_release_date.gte\"] = date_range[0]\n",
    "        params[\"primary_release_date.lte\"] = date_range[1]\n",
    "\n",
    "    for p in range(1, pages + 1):\n",
    "        logger.info(f\"--- Discover page {p} --- (range={date_range})\")\n",
    "        params[\"page\"] = p\n",
    "        r = fetch(\"/discover/movie\", params)\n",
    "        results.extend(r.get(\"results\", []))\n",
    "\n",
    "        # stop early if last page\n",
    "        if p >= r.get(\"total_pages\", 1):\n",
    "            break\n",
    "\n",
    "        time.sleep(0.1)  # respect rate-limit\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b5356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single movie and append details to ndjson file\n",
    "def process_movie(movie_id, out_file):\n",
    "    try:\n",
    "        data = fetch(f\"/movie/{movie_id}\",\n",
    "                     # Fetch movie details with credits and reviews\n",
    "                     {\"append_to_response\": \"credits,reviews\"})\n",
    "        with open(out_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            # Append movie data to ndjson file\n",
    "            f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed {movie_id}: {e}\")\n",
    "        return movie_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b22a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process list of IDs concurrently\n",
    "\n",
    "def process_ids(movie_ids, out_file, workers=16):  # number of concurrent workers to use\n",
    "    failed = []\n",
    "    # Use ThreadPoolExecutor for I/O bound tasks\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        futures = [executor.submit(process_movie, mid, out_file)  # Submit tasks to executor\n",
    "                   for mid in movie_ids]\n",
    "        # Process completed futures\n",
    "        for fut in concurrent.futures.as_completed(futures):\n",
    "            res = fut.result()\n",
    "            if res:\n",
    "                failed.append(res)\n",
    "    return failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8503cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load existing IDs from ndjson file to avoid duplicates\n",
    "def load_existing_ids(out_file):\n",
    "    existing_ids = set()\n",
    "    if os.path.exists(out_file):\n",
    "        with open(out_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    if \"id\" in data:\n",
    "                        existing_ids.add(data[\"id\"])\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    return existing_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e0646",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main pipeline function\n",
    "\n",
    "def main(pages=500, workers=16):\n",
    "    all_movies = []\n",
    "    seen = load_existing_ids(OUT_FILE)\n",
    "\n",
    "    logger.info(f\"Resume mode: Found {len(seen)} movies already saved âœ…\")\n",
    "\n",
    "    # Read values from .env\n",
    "    step = os.getenv(\"DATE_STEP\", \"year\")  # \"year\" or \"half\"\n",
    "    start_year = int(os.getenv(\"START_YEAR\", \"1900\"))\n",
    "    end_year = int(os.getenv(\"END_YEAR\", \"2025\"))\n",
    "\n",
    "    # ranges = generate_date_ranges(start_year, end_year, step=step)\n",
    "    # ranges = generate_quarter_ranges(start_year, end_year)\n",
    "    ranges = generate_monthly_ranges(start_year, end_year)\n",
    "\n",
    "    for start, end in ranges:\n",
    "        logger.info(f\"Fetching movies between {start} and {end}\")\n",
    "        movies = discover_movies(pages=pages, date_range=(start, end))\n",
    "        ids = [m[\"id\"] for m in movies if m[\"id\"] not in seen]\n",
    "        seen.update(ids)\n",
    "        all_movies.extend(ids)\n",
    "        logger.info(f\"Collected {len(ids)} new movies for {start} â†’ {end}\")\n",
    "\n",
    "        # Save all into one file\n",
    "        failed_ids = process_ids(ids, OUT_FILE, workers)\n",
    "\n",
    "        # Retry failed IDs\n",
    "        if failed_ids:\n",
    "            logger.info(f\"Retrying {len(failed_ids)} failed movies...\")\n",
    "            retry_failed = process_ids(failed_ids, OUT_FILE, workers)\n",
    "            if retry_failed:\n",
    "                fail_file = os.path.join(OUT_DIR, \"failed_ids.txt\")\n",
    "                with open(fail_file, \"w\") as f:\n",
    "                    f.write(\"\\n\".join(map(str, retry_failed)))\n",
    "                logger.info(\n",
    "                    f\"Still failed after retry. Saved {len(retry_failed)} IDs to {fail_file}\")\n",
    "            else:\n",
    "                logger.info(\"All failed IDs succeeded on retry âœ…\")\n",
    "\n",
    "    logger.info(f\"Total new movies downloaded: {len(all_movies)}\")\n",
    "    logger.info(f\"Final data saved in {OUT_FILE}\")\n",
    "    logger.info(\"Pipeline finished!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55a57a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = r\"D:\\depi_project\\data\\raw\\movies_all.ndjson\"   # file Large JSON\n",
    "output_file = r\"D:\\depi_project\\data\\raw\\movies_all.csv\"  # output CSV file\n",
    "\n",
    "# write CSV in chunks\n",
    "chunksize = 100000\n",
    "batch = []\n",
    "count = 0\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f, open(output_file, \"w\", encoding=\"utf-8\", newline=\"\") as out_csv:\n",
    "    writer = None\n",
    "\n",
    "    for line in f:\n",
    "        try:\n",
    "            record = json.loads(line.strip())  # convert line to dict\n",
    "            batch.append(record)\n",
    "        except json.JSONDecodeError:\n",
    "            continue  # If a line is corrupted, skip it\n",
    "\n",
    "        if len(batch) >= chunksize:\n",
    "            df = pd.DataFrame(batch)\n",
    "\n",
    "            if writer is None:\n",
    "                df.to_csv(out_csv, index=False, header=True)\n",
    "                writer = True\n",
    "            else:\n",
    "                df.to_csv(out_csv, index=False, header=False)\n",
    "\n",
    "            count += len(batch)\n",
    "            print(f\"âœ… Processed {count:,} records...\")\n",
    "            batch = []\n",
    "\n",
    "    # Write remaining records\n",
    "    if batch:\n",
    "        df = pd.DataFrame(batch)\n",
    "        if writer is None:\n",
    "            df.to_csv(out_csv, index=False, header=True)\n",
    "        else:\n",
    "            df.to_csv(out_csv, index=False, header=False)\n",
    "        count += len(batch)\n",
    "\n",
    "print(f\"ðŸŽ‰ Finished! Total {count:,} records saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c7bc97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
